txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
INDEX = sample(1:nrow(dat_json), 1)
txt = dat_json$name[INDEX]
txt
vec_json_nam = init_mod$vector_embedding(text_sentence = txt, reduce_function = REDUCE_METHOD)
vec_json_nam
dat = as.matrix(df_vec_all)
dimnames(dat) = NULL
embed_matrix
embed_matrix = matrix(vec_json_nam, nrow = 1, ncol = length(vec_json_nam))
embed_matrix
dat_quer = rbind(embed_matrix, dat)
text_embeds = c(txt, preds_out_dtbl$skills)
nmslib_idx = shiny_nmslib_index(data_index = dat_quer,
nmslib_M = 30,
nmslib_efC = 100,
nmslib_efS = 100,
nmslib_space_name = 'cosinesimil',
nmslib_method = 'hnsw',
threads = 6,
verbose = TRUE)
nmslib_k = 20
single_query = nmslib_idx$Knn_Query(query_data_row = dat_quer[1, ], k = nmslib_k)
dtbl_res = data.table::setDT(list(dist = as.vector(single_query[[2]]), skill_job_desciption = text_embeds[single_query[[1]]]))
dtbl_res
txt
dat_json
dat_json[sample(1:nrow(dat_json),20), ]
dat_json[sample(1:nrow(dat_json),20), ]
dat_json[sample(1:nrow(dat_json),20), ]
dat_json[sample(1:nrow(dat_json),20), ]
dat_json[sample(1:nrow(dat_json),20), ]
dat_json[sample(1:nrow(dat_json),20), ]
dat_json[sample(1:nrow(dat_json),20), ]
dat_json[sample(1:nrow(dat_json),20), ]
preds_out_dtbl[sample(1:nrow(preds_out_dtbl),20), ]
dat_json[sample(1:nrow(dat_json),20), ]
dat_json[sample(1:nrow(dat_json),20), ]
dat_json[sample(1:nrow(dat_json),20), ]
dat_json
pth_json = '/home/lampros/Desktop/freelancher_platforms/Upwork_Freelancer_PROPOSALS/upwork_proposal_skill_extractor_NLP_Spacy_29_01_2021/skills-1611849302020.json'
dat_json = jsonlite::fromJSON(pth_json)
dat_json
head(dat_json, 20)
1
res = DBI::dbSendQuery(con, "copy (select * from tigris_2019_address limit 10) to '/home/lampros/Desktop/freelancher_platforms/geospatial/Geofencing_US_Census_Data/gdb_to_csv/US_Census_Address_LIMIT.csv' with csv header")
con = DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "tigris_2019_address",                            # I have first to create the database in the terminal then to call the database here
host = "localhost",
port = 5432,
user = 'postgres',
password = '691976')
res = DBI::dbSendQuery(con, "copy (select * from address_ranges limit 10) to '/home/lampros/Desktop/freelancher_platforms/geospatial/Geofencing_US_Census_Data/gdb_to_csv/US_Census_Address_LIMIT.csv' with csv header")
q_exec = DBI::dbExecute(con, statement = "copy (select * from address_ranges limit 10) to '/home/lampros/Desktop/freelancher_platforms/geospatial/Geofencing_US_Census_Data/gdb_to_csv/US_Census_Address_LIMIT.csv' with csv header")
con = DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "tigris_2019_address",                            # I have first to create the database in the terminal then to call the database here
host = "localhost",
port = 5432,
user = 'postgres',
password = '691976')
1
q_exec = DBI::dbExecute(con, statement = "copy (select * from address_ranges limit 10) to '/home/lampros/Desktop/freelancher_platforms/geospatial/Geofencing_US_Census_Data/gdb_to_csv/US_Census_Address_LIMIT.csv' with csv header")
DBI::dbDisconnect(con)
download.file('https://data.census.gov/api/access/table/download?download_id=044Rd3cBDOM0J-kZFYS_', destfile = '/home/lampros/Downloads/DECENIAL_TMP.json')
s = jsonlite::fromJSON('/home/lampros/Downloads/DECENNIALSF12010.H10_2021-02-08T072421.json')
s = jsonlite::fromJSON('/home/lampros/Downloads/DECENIAL_TMP.json')
download.file('https://data.census.gov/api/access/table/download?download_id=lOKjgXcBMY-sQw3V5FN8', destfile = '/home/lampros/Downloads/DECENIAL_TMP.zip')
download.file('https://data.census.gov/api/access/table/download?download_id=lOKjgXcBMY-sQw3V5FN8', destfile = '/home/lampros/Downloads/DECENIAL_TMP.zip')
#############################################################
### To DEBUG uncomment the 'ROOT_REPOSITORY' variable     ###
### If you want to run in my PC then set the THREADS to 8 ###
### Moreover replace the batch-code with the commented    ###
### out snippet beginning in line 103                     ###
#############################################################
#****************************************************************************************************************************************************
# 1st. section: Update my BigQuery table with any missing products from the "Open AWS-S3-registry". There might be many reasons for missing products:
#
#               - late uploading from scihub
#               - my regular cron-job runs once per day and captures only the products at this specific time (late products are not included)
#               - etc.
#****************************************************************************************************************************************************
# !!!!!!!!!!!  IMPORTANT   !!!!!!!!!  It seems that there are no costs associated with uploading data to a BigQuery table
#                                     See the following web-links for more information:
#
#                                     - https://cloud.google.com/bigquery/quotas#load_jobs
#                                     - https://cloud.google.com/bigquery/pricing#free
#                                     - https://cloud.google.com/bigquery/docs/loading-data
#****************************************************************************************************************************************************
#...................................
# ROOT_REPOSITORY = '/root'                                                # this is the directory that I mount in the 'Jenkinsfile' ( SEE: args '-v /home/mouselimislampros_gmail_com/:/root/' at the beginning of the Jenkinsfile pipeline )
# THREADS = 2
ROOT_REPOSITORY = Sys.getenv('HOME')
# THREADS = 5                                       # if I have approx. 90.000 observation use fewer threads
THREADS = 8
#...................................
PATH_OAUTH = file.path(ROOT_REPOSITORY, 'oauth2_creds_yagmail.json')
PATH_GARGLE = file.path(ROOT_REPOSITORY, ".R/gargle/gargle-oauth")
cat("BigQuery Authentication / Credentials ...\n")
bigquery_auth_params = list(bq_auth_list = list(email = 'mouselimislampros@gmail.com',
path = PATH_OAUTH),
gargle_dir_path = PATH_GARGLE)
cat('\n')
cat(glue::glue("Does the 'oauth2_path' file exist? {file.exists(PATH_OAUTH)}"), '\n')
cat(glue::glue("Does the 'gargle_path' directory exist? {dir.exists(PATH_GARGLE)}"), '\n')
cat('\n')
cat("Enable Authentication for BigQuery using 'options()' and 'gargle_oauth_email' ...\n")
options(gargle_oauth_cache = bigquery_auth_params[['gargle_dir_path']], gargle_oauth_email = bigquery_auth_params[['bq_auth_list']][['email']])
cat("Bigquery authentication ...\n")
do.call(bigrquery::bq_auth, bigquery_auth_params[['bq_auth_list']])
cat("First extract the available days of the Sentinel-2ARD products ..\n")
av_days = sentinel.sentinel2ard:::extract_available_days(base_s2ard_url = 's3://sentinel-s2-l2a',
verbose = TRUE)
print(str(av_days$dates_and_weblinks))
print(str(av_days$only_dates))
#..........................................................................................................
# In case that I want to process the products of years >= 2019 and based on these products to create the
# BigQuery table then use the following dates in a for-loop ('extract_single_day_S2_ARD_products' function)
#..........................................................................................................
cat("process the products of years >= 2019 ..\n")
CURRENT_YEAR = strftime(Sys.Date(), format = '%Y')                  # Extract the current year
CURRENT_YEAR = as.integer(CURRENT_YEAR)
restrict_search_years = seq(2019, CURRENT_YEAR, 1)                  # Restrict the search of products to years >= '2019'
vec_days_19_and_20 = list()
for (iter_year in 1:length(restrict_search_years)) {
cat(glue::glue("Year:  {restrict_search_years[iter_year]}  will be added to the list-object ..."), '\n')
vec_days_19_and_20[[iter_year]] = unlist(av_days$only_dates[[as.character(restrict_search_years[iter_year])]])
}
vec_days_19_and_20 = as.vector(unlist(vec_days_19_and_20))
print(length(vec_days_19_and_20))
cat("Then based on the output days compute the available products ...\n")
prods = sentinel.sentinel2ard:::extract_available_products(dates_vec = vec_days_19_and_20,
base_s3_url = 's3://sentinel-s2-l2a',
threads = THREADS,                             # use a single thread if run this function in my cloud instance
verbose = TRUE)
print(str(prods[1:5]))
cat('Build a data.table from the output list objects  ( as of "02-05-2020" it returns 5.318.037  Mio. products for a 1 and 1/2 year time interval ) ...\n')
prods_dtbl = data.table::rbindlist(prods)
print(dim(prods_dtbl))
#  !!!!!!!!!!!!! USE THE FOLLOWING COMMENTED OUT CODE SNIPPET IN CASE THAT I RECEIVE AN ERROR ON MY CLOUD INSTANCE  !!!!!!!!!!   [ IT REPLACES THE FOLLOWING BATCH-FOR-LOOP ]
# #*******************************************************************************************************  This gives a memory error because I have to download (upload into R) more than 5.000.000 Mio. rows at once
cat("once I have the 'prods_dtbl' compare the 'products' with the corresponding column of the BigQuery table ...\n")
sql = "SELECT product_name FROM `gc-project-242606.sentinel_2.sentinel_2_ARD`"
res = SentinelBigQuery::BigQuery(SQL_query = sql,
project = 'gc-project-242606',
# DATASET = 'gc-project-242606:sentinel_2.sentinel_2_ARD',
DATASET = 'sentinel_2',
bq_auth_list = list(email = bigquery_auth_params$bq_auth_list$email,
path = bigquery_auth_params$bq_auth_list$path),
# max_pages = Inf,                                                            # I updated the 'SentinelBigQuery' package at "17-06-2020"
max_results = Inf,
verbose = TRUE)
dif_prods = setdiff(prods_dtbl$products, res$product_name)
idx_download = which(prods_dtbl$products %in% dif_prods)
dat_download = prods_dtbl[idx_download, , drop = F]
print(dim(dat_download))
# # #*******************************************************************************************************  Run the following in the cloud instance
#
THREADS = 2
cat("Once I have product-names  &&  dates  of the difference in products proceed with the processing of the remaining products  ( use the 'extract_single_day_S2_ARD_products()' for this purpose ) ...\n")
temp_file_save = tempfile(fileext = '.fst')
# temp_file_RDATA = tempfile(fileext = '.RDATA')
res = sentinel.sentinel2ard:::extract_single_day_S2_ARD_products(base_s2ard_url = 's3://sentinel-s2-l2a',
date = 'NA/NA/NA/',
missing_products_data_table = dat_download,               # !!!! By specifying the data.table with the remaining products I ignore the 'date' parameter  !!!!
productInfo_file = 'productInfo.json',
tileInfo_file = 'tileInfo.json',
threads = THREADS,                                              # or 8, 2 etc. threads  ( be aware of memory usage because 'mclapply' will be used )
write_file = temp_file_save,
fst_write_compress = 50,
debugging_mode_RDATA_path = NULL,
# debugging_mode_RDATA_path = temp_file_RDATA,             # set this parameter to a valid path to an .RDATA object to use a regular for-loop, to save() the for-loop data and increase the verbosity (it prints: iteration && product-name)
file_path_keep_track_dates = NULL,                         # applies only if I download 1-day's products
verbose = TRUE)
pth_dtbl = '/home/lampros/Desktop/freelancher_platforms/geospatial/Point_Mapping_Clustering/EnLink_Master_File_from_ArcGIS.csv'
dtbl = data.table::fread(pth_dtbl, header = T, stringsAsFactors = T, nThread = parallel::detectCores())
dtbl
COLS = c('PointType', 'Latitude', 'Longitude')
dtbl_subs = dtbl[, ..COLS]
colnames(dtbl_subs) = c('PointType', 'latitude', 'longitude')
dtbl_subs
SKLEARN = reticulate::import('sklearn.cluster', convert = FALSE)
NP = reticulate::import('numpy', convert = FALSE)
cols_ll = c('longitude', 'latitude')
dat_lat_lon = dtbl_subs[, ..cols_ll]
dat_lat_lon = as.matrix(dat_lat_lon)
dimnames(dat_lat_lon) = NULL
mt_np = reticulate::np_array(dat_lat_lon)
rads = NP$radians(mt_np)
dist_betw_pnts = 50.0
min_samp = 5
threads = 8
db_cl = SKLEARN$DBSCAN(eps = dist_betw_pnts / 6371.0,
min_samples = as.integer(min_samp),
algorithm = 'ball_tree',
metric = 'haversine',
n_jobs = as.integer(threads))$fit(rads)
#...................................................................................................................................... INFO
# Perform an analysis to cluster these data points into groups of approximately 25-35 points each, based on proximity.
# Include the ability to FILTER or ADJUST GROUP MEMBERSHIP on one or more dimensions (LOCATION TYPE / SERVICE PERFORMED).
#......................................................................................................................................
#.....................
# work with data.table
#.....................
# pth = '/home/lampros/Desktop/freelancher_platforms/geospatial/Point_Mapping_Clustering/EnLink Master File from ArcGIS.xlsx'
# dat_xls = openxlsx::read.xlsx(pth, sheet = 1)
# dim(dat_xls)
# head(dat_xls)
pth_dtbl = '/home/lampros/Desktop/freelancher_platforms/geospatial/Point_Mapping_Clustering/EnLink_Master_File_from_ArcGIS.csv'
# data.table::fwrite(dat_xls, pth_dtbl, row.names = F, nThread = parallel::detectCores())
dtbl = data.table::fread(pth_dtbl, header = T, stringsAsFactors = T, nThread = parallel::detectCores())
dtbl
# table(dtbl$PointType)
# length(unique(dtbl$LocationDescription))
# # 27.416
# length(unique(dtbl$PipelineID))
# # 6.987
# length(unique(dtbl$AdditionalInfo))
# # 13.423
# table(dtbl$OperationalStatus)
# # 10
# length(unique(dtbl$Remarks))
# # 7.522
COLS = c('PointType', 'Latitude', 'Longitude')
dtbl_subs = dtbl[, ..COLS]
colnames(dtbl_subs) = c('PointType', 'latitude', 'longitude')
dtbl_subs
#........................
# Custom Leaflet function
#........................
VIZ_WEB_GL_points = function(dat_dtbl,
provider = leaflet::providers$Esri.WorldImagery,
option_viewer = rstudioapi::viewer,
CRS = 4326,
use_leaflet = FALSE,
COLOR = NULL,
popup = NULL,                                        # or 'Temperature'
extended_bbox_WKT = NULL) {
if (use_leaflet) {
lat = dat_dtbl$latitude
lon = dat_dtbl$longitude
}
# if (!is.null(popup)) {
#   subs = as.matrix(dat_dtbl[, ..popup])
#   norm_temp = OpenImageR::norm_matrix_range(subs, min_value = 1, max_value = 100)[, 1]
#   heat_pal = leaflet::colorNumeric(palette = as.character(grDevices::heat.colors(n = 9, alpha = 1, rev = TRUE)), domain = norm_temp)
# }
dat_dtbl = sf::st_as_sf(dat_dtbl, coords = c("longitude", "latitude"))          # create a simple feature from lat, lon
dat_dtbl = sf::st_set_crs(dat_dtbl, CRS)
if (!is.null(extended_bbox_WKT)) {
bbox_vec = sf::st_as_sfc(extended_bbox_WKT)
bbox_vec = sf::st_set_crs(bbox_vec, CRS)
bbox_vec = sf::st_bbox(bbox_vec)
bbox_vec = as.vector(bbox_vec)
leaflet_fitBounds_bbox = list(xmin = bbox_vec[1],
ymin = bbox_vec[2],
xmax = bbox_vec[3],
ymax = bbox_vec[4])
}
else {
bbox_vec = sf::st_bbox(dat_dtbl)
bbox_vec = as.vector(bbox_vec)
leaflet_fitBounds_bbox = list(xmin = bbox_vec[1],
ymin = bbox_vec[2],
xmax = bbox_vec[3],
ymax = bbox_vec[4])
}
options(viewer = option_viewer)
lft = leaflet::leaflet()
lft = leaflet::addProviderTiles(map = lft,
provider = provider)
if (use_leaflet) {
COLOR = "#03F"
RADIUS = 5
POPUP = NULL
if (!is.null(popup)) {
COLOR = heat_pal(norm_temp)
# POPUP = as.character(subs[, 1])
# RADIUS = norm_temp
}
lft = leaflet::addCircleMarkers(map = lft,
lng = lon,
lat = lat,
radius = RADIUS,
color = COLOR,
popup = POPUP,
stroke = FALSE, fillOpacity = 0.9)
}
else {
# COLOR = cbind(0, 0.2, 1)
# if (!is.null(popup)) {
# COLOR = as.vector(subs[, 1])
# }
lft = leafgl::addGlPoints(map = lft,
data = dat_dtbl,
opacity = 1.0,
color = COLOR,
popup = popup)
}
def_lft = leaflet::fitBounds(map = lft,
lng1 = leaflet_fitBounds_bbox$xmin,
lat1 = leaflet_fitBounds_bbox$ymin,
lng2 = leaflet_fitBounds_bbox$xmax,
lat2 = leaflet_fitBounds_bbox$ymax)
return(def_lft)
}
res = VIZ_WEB_GL_points(dat_dtbl = dtbl_subs,
provider = leaflet::providers$CartoDB.Positron,
option_viewer = NULL,                                    # open the visualization in the web-browser
CRS = 4326,
use_leaflet = FALSE,
COLOR = 'PointType',
popup = 'PointType',
extended_bbox_WKT = NULL)
res
cols_ll = c('longitude', 'latitude')
dat_lat_lon = dtbl_subs[, ..cols_ll]
dat_lat_lon = as.matrix(dat_lat_lon)
dat_lat_lon
km = ClusterR::KMeans_rcpp(data = dat_lat_lon, clusters = 8,
num_init = 1,
max_iters = 100,
initializer = "kmeans++",
fuzzy = FALSE,
verbose = TRUE,
CENTROIDS = NULL,
tol = 1e-04,
tol_optimal_init = 0.3,
seed = 1)
str(km)
table(km$clusters)
dtbl_subs
km$centroids
d = data.table::data.table(km$centroids)
d
d$PointType = rep('CENTROIDS', nrow(d))
d
colnames(d) = c('longitude', 'latitude', 'PointType')
s = c('PointType', 'latitude', 'longitude')
d = d[, ..s]
d
dtbl_subs1 = rbind(dtbl_subs, d)
dtbl_subs1
res1 = VIZ_WEB_GL_points(dat_dtbl = dtbl_subs1,
provider = leaflet::providers$CartoDB.Positron,
option_viewer = NULL,                                    # open the visualization in the web-browser
CRS = 4326,
use_leaflet = FALSE,
COLOR = 'PointType',
popup = 'PointType',
extended_bbox_WKT = NULL)
res1
km = ClusterR::KMeans_rcpp(data = dat_lat_lon,
clusters = 7,
num_init = 1,
max_iters = 100,
initializer = "kmeans++",
fuzzy = FALSE,
verbose = TRUE,
CENTROIDS = NULL,
tol = 1e-04,
tol_optimal_init = 0.3,
seed = 1)
str(km)
table(km$clusters)
d = data.table::data.table(km$centroids)
d$PointType = rep('CENTROIDS', nrow(d))
colnames(d) = c('longitude', 'latitude', 'PointType')
s = c('PointType', 'latitude', 'longitude')
d = d[, ..s]
d
dtbl_subs1 = rbind(dtbl_subs, d)
res1 = VIZ_WEB_GL_points(dat_dtbl = dtbl_subs1,
provider = leaflet::providers$CartoDB.Positron,
option_viewer = NULL,                                    # open the visualization in the web-browser
CRS = 4326,
use_leaflet = FALSE,
COLOR = 'PointType',
popup = 'PointType',
extended_bbox_WKT = NULL)
res1
km = ClusterR::KMeans_rcpp(data = dat_lat_lon,
clusters = 6,
num_init = 1,
max_iters = 100,
initializer = "kmeans++",
fuzzy = FALSE,
verbose = TRUE,
CENTROIDS = NULL,
tol = 1e-04,
tol_optimal_init = 0.3,
seed = 1)
str(km)
table(km$clusters)
d = data.table::data.table(km$centroids)
d$PointType = rep('CENTROIDS', nrow(d))
colnames(d) = c('longitude', 'latitude', 'PointType')
s = c('PointType', 'latitude', 'longitude')
d = d[, ..s]
d
dtbl_subs1 = rbind(dtbl_subs, d)
res1 = VIZ_WEB_GL_points(dat_dtbl = dtbl_subs1,
provider = leaflet::providers$CartoDB.Positron,
option_viewer = NULL,                                    # open the visualization in the web-browser
CRS = 4326,
use_leaflet = FALSE,
COLOR = 'PointType',
popup = 'PointType',
extended_bbox_WKT = NULL)
res1
km = ClusterR::KMeans_rcpp(data = dat_lat_lon,
clusters = 7,
num_init = 1,
max_iters = 100,
initializer = "kmeans++",
fuzzy = FALSE,
verbose = TRUE,
CENTROIDS = NULL,
tol = 1e-04,
tol_optimal_init = 0.3,
seed = 1)
str(km)
table(km$clusters)
d = data.table::data.table(km$centroids)
d$PointType = rep('CENTROIDS', nrow(d))
colnames(d) = c('longitude', 'latitude', 'PointType')
s = c('PointType', 'latitude', 'longitude')
d = d[, ..s]
d
dtbl_subs1 = rbind(dtbl_subs, d)
res1 = VIZ_WEB_GL_points(dat_dtbl = dtbl_subs1,
provider = leaflet::providers$CartoDB.Positron,
option_viewer = NULL,                                    # open the visualization in the web-browser
CRS = 4326,
use_leaflet = FALSE,
COLOR = 'PointType',
popup = 'PointType',
extended_bbox_WKT = NULL)
res1
setwd('/home/lampros/add_GITHUB/KernelKnn')
tic::use_tic()
